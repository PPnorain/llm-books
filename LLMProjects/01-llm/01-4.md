大型语言模型是人工智能的一个子集，它使用深度学习和神经网络来处理自然语言。Transformer是一种神经网络架构，可以使用自注意机制学习序列数据中的上下文。它们于2017年由Google Brain的一个团队引入，并已成为LLM研究的热门。基于变换器的LLM的一些示例是BERT、GPT-3、T5和Megatron-LM。

